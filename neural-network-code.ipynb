{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, Dense, Dropout, Lambda, concatenate\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Explanatory Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1986,2017):\n",
    "             vars()[\"year_\"+str(i)]=pd.read_csv('Desktop/data-python/experiment//climate-Historical/avg_climate_'+str(i)+'.csv')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2062,2093):\n",
    "                  vars()[\"year_\"+str(i)]=pd.read_csv('Desktop/data-python/experiment/climate-Future//avg_climate_'+str(i)+'.csv')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1986,2017):\n",
    "        del globals()[((\"year_\"+str(i)))]['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2062,2093):\n",
    "           del globals()[((\"year_\"+str(i)))]['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "for i in range(1986,2017):\n",
    "        vars()[\"scalerX\"+str(i)]=StandardScaler().fit(globals()[\"year_\"+str(i)])\n",
    "        vars()[\"scaled\"+str(i)]=vars()[\"scalerX\"+str(i)].transform(globals()[\"year_\"+str(i)])\n",
    "        vars()[\"year_\"+str(i)+\"_threeD\"]=np.expand_dims(vars()[\"scaled\"+str(i)],axis=1)\n",
    "    \n",
    "for i in range(2062,2093):\n",
    "            vars()[\"scalerX\"+str(i)]=StandardScaler().fit(globals()[\"year_\"+str(i)])\n",
    "            vars()[\"scaled\"+str(i)]=vars()[\"scalerX\"+str(i)].transform(globals()[\"year_\"+str(i)])\n",
    "            vars()[\"year_\"+str(i)+\"_threeD\"]=np.expand_dims(vars()[\"scaled\"+str(i)],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Test Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_d_array_input_train1=np.concatenate([year_1986_threeD])\n",
    "for i in range(1987,2017):\n",
    "   \n",
    "        if(i not in [1991,1992,1998,1999,2005,2006]):\n",
    "            three_d_array_input_train1=np.concatenate([three_d_array_input_train1,globals()[\"year_\"+str(i)+\"_threeD\"]],axis=1)\n",
    "\n",
    "\n",
    "three_d_array_input_train2=np.concatenate([year_2062_threeD])\n",
    "for i in range(2063,2093):\n",
    "            if(i not in [2065,2070,2072,2076,2086,2091]):\n",
    "                three_d_array_input_train2=np.concatenate([three_d_array_input_train2,globals()[\"year_\"+str(i)+\"_threeD\"]],axis=1)\n",
    "                \n",
    "three_d_array_input_train=np.concatenate([three_d_array_input_train1,three_d_array_input_train2],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_d_array_input_test1=np.concatenate([year_1991_threeD])\n",
    "for i in range(1987,2017):\n",
    "            if(i in [1992,1998,1999,2005,2006]):\n",
    "                three_d_array_input_test1=np.concatenate([three_d_array_input_test1,globals()[\"year_\"+str(i)+\"_threeD\"]],axis=1)\n",
    "\n",
    "three_d_array_input_test2=np.concatenate([year_2065_threeD])\n",
    "for i in range(2062,2093):\n",
    "        if(i in [2070,2072,2076,2086,2091]):\n",
    "            three_d_array_input_test2=np.concatenate([three_d_array_input_test2,globals()[\"year_\"+str(i)+\"_threeD\"]],axis=1)\n",
    "\n",
    "\n",
    "three_d_array_input_test=np.concatenate([three_d_array_input_test1,three_d_array_input_test2,three_d_array_input_test1,\n",
    "                                        three_d_array_input_test1,three_d_array_input_test1,three_d_array_input_test1,\n",
    "                                        three_d_array_input_test1,three_d_array_input_test1,year_1986_threeD,year_1987_threeD],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_d_array_input_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_d_array_input_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1986,2017):\n",
    "            vars()[\"year_\"+str(i)+\"_response\"]=pd.read_csv('Desktop/data-python/experiment/response-Historical/avg_response_'+str(i)+'.csv')[47:17521]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2062,2093):\n",
    "            vars()[\"year_\"+str(i)+\"_response\"]=pd.read_csv('Desktop/data-python/experiment/response-Future/avg_response_'+str(i)+'.csv')[47:17521]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1986,2017):\n",
    "              del globals()[((\"year_\"+str(i)+\"_response\"))]['Unnamed: 0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2062,2093):\n",
    "              del globals()[((\"year_\"+str(i)+\"_response\"))]['Unnamed: 0']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "for i in range(1986,2017):\n",
    "    vars()[\"year_\"+str(i)+\"_threeD\"+\"_response\"]=np.expand_dims(vars()[\"year_\"+str(i)+\"_response\"],axis=1)\n",
    "    \n",
    "for i in range(2062,2093):\n",
    "    vars()[\"year_\"+str(i)+\"_threeD\"+\"_response\"]=np.expand_dims(vars()[\"year_\"+str(i)+\"_response\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler ,MinMaxScaler\n",
    "\n",
    "for i in range(1986,2017):\n",
    "        vars()[\"scalerY\"+str(i)]=StandardScaler().fit(globals()[\"year_\"+str(i)+\"_response\"])\n",
    "        vars()[\"scaled\"+str(i)+\"_response\"]=vars()[\"scalerY\"+str(i)].transform(globals()[\"year_\"+str(i)+\"_response\"])\n",
    "        vars()[\"year_\"+str(i)+\"_threeD\"+\"_response\"]=np.expand_dims(vars()[\"scaled\"+str(i)+\"_response\"],axis=1)\n",
    "    \n",
    "for i in range(2062,2093):\n",
    "            vars()[\"scalerY\"+str(i)]=StandardScaler().fit(globals()[\"year_\"+str(i)+\"_response\"])\n",
    "            vars()[\"scaled\"+str(i)+\"_response\"]=vars()[\"scalerY\"+str(i)].transform(globals()[\"year_\"+str(i)+\"_response\"])\n",
    "            vars()[\"year_\"+str(i)+\"_threeD\"+\"_response\"]=np.expand_dims(vars()[\"scaled\"+str(i)+\"_response\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Test Response Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_d_array_output_train1=np.concatenate([year_1986_threeD_response])\n",
    "for i in range(1987,2017):\n",
    "          if(i not in [1991,1992,1998,1999,2005,2006]):\n",
    "                three_d_array_output_train1=np.concatenate([three_d_array_output_train1,globals()[\"year_\"+str(i)+\"_threeD\"+\"_response\"]],axis=1)\n",
    "\n",
    "three_d_array_output_train2=np.concatenate([year_2062_threeD_response])\n",
    "for i in range(2063,2093):\n",
    "            if(i not in [2065,2070,2072,2076,2086,2091]):\n",
    "                three_d_array_output_train2=np.concatenate([three_d_array_output_train2,globals()[\"year_\"+str(i)+\"_threeD\"+\"_response\"]],axis=1)\n",
    "\n",
    "        \n",
    "three_d_array_output_train=np.concatenate([three_d_array_output_train1,three_d_array_output_train2],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_d_array_output_test1=np.concatenate([year_1991_threeD_response])\n",
    "for i in range(1987,2017):\n",
    "            if(i in [1992,1998,1999,2005,2006]):\n",
    "                three_d_array_output_test1=np.concatenate([three_d_array_output_test1,globals()[\"year_\"+str(i)+\"_threeD\"+\"_response\"]],axis=1)\n",
    "         \n",
    "        \n",
    "three_d_array_output_test2=np.concatenate([year_2065_threeD_response]) \n",
    "for i in range(2062,2093):\n",
    "            if(i in [2070,2072,2076,2086,2091]):\n",
    "                    three_d_array_output_test2=np.concatenate([three_d_array_output_test2,globals()[\"year_\"+str(i)+\"_threeD\"+\"_response\"]],axis=1)\n",
    "\n",
    "three_d_array_output_test=np.concatenate([three_d_array_output_test1,three_d_array_output_test2,three_d_array_output_test1,\n",
    "                                         three_d_array_output_test1,three_d_array_output_test1,three_d_array_output_test1,\n",
    "                                          three_d_array_output_test1,three_d_array_output_test1\n",
    "                                          ,year_1986_threeD_response,year_1987_threeD_response],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_d_array_output_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_d_array_output_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL SETUP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaveNet (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, Dense, Activation, Dropout, Lambda, Multiply, Add, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "dilation_rates = [2**i for i in range(4)]*3\n",
    "\n",
    "Input_Data = Input(shape=(50, 4))\n",
    "x = Input_Data\n",
    "x = Conv1D(128, 1, padding='causal')(x) \n",
    "skips = []\n",
    "for dilation_rate in dilation_rates:\n",
    "    \n",
    "    # filter convolution\n",
    "    x_f = Conv1D(filters=4,\n",
    "                 kernel_size=128, \n",
    "                 padding='causal',\n",
    "                 dilation_rate=dilation_rate)(x)\n",
    "    \n",
    "    x_g = Conv1D(filters=4,\n",
    "                 kernel_size=128,\n",
    "                 padding='causal',\n",
    "                 dilation_rate=dilation_rate)(x)\n",
    "        \n",
    "    # multiply filter and gating branches\n",
    "    z = Multiply()([Activation('tanh')(x_f),\n",
    "                    Activation('sigmoid')(x_g)])\n",
    "    \n",
    "    \n",
    "    z = Conv1D(128, 1, padding='same')(z)\n",
    "    \n",
    "    # residual connection\n",
    "    x = Add()([x, z])    \n",
    "    \n",
    "    # collect skip connections\n",
    "    skips.append(z)\n",
    "\n",
    "# add all skip connection outputs \n",
    "out = Activation('relu')(Add()(skips))\n",
    "\n",
    "# final time-distributed dense layers \n",
    "out = Conv1D(128, 1, padding='same')(out)\n",
    "out = Activation('relu')(out)\n",
    "out = Conv1D(1, 1, padding='same')(out)\n",
    "model = Model(history_seq, out)\n",
    "model.compile(Adam(), loss='mean_absolute_error')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import LSTM,GRU\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, activation='relu', input_shape=(50,192),return_sequences=True))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='Adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameters Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "class prediction_history(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.predhis=(model.predict(three_d_array_input_train))\n",
    "        \n",
    "predictions=prediction_history()\n",
    "callbacks=[predictions]\n",
    "\n",
    "stop = keras.callbacks.callbacks.EarlyStopping(monitor='loss',\n",
    "                           min_delta=0.001,\n",
    "                           patience=3,\n",
    "                           verbose=1,\n",
    "                           mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Validation Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(three_d_array_input_train, three_d_array_output_train,batch_size=batch_size,epochs=epochs,\n",
    "          validation_data=(three_d_array_input_test,three_d_array_output_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model1.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# model = load_model('heavy-act3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(three_d_array_input_test,three_d_array_output_test,batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_scaled=model.predict(three_d_array_input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y1991=yhat_scaled[0:17473,0,0]\n",
    "y1992=yhat_scaled[0:17473,1,0]\n",
    "y1998=yhat_scaled[0:17473,2,0]\n",
    "y1999=yhat_scaled[0:17473,3,0]\n",
    "y2005=yhat_scaled[0:17473,4,0]\n",
    "y2006=yhat_scaled[0:17473,5,0]\n",
    "y2065=yhat_scaled[0:17473,6,0]\n",
    "y2070=yhat_scaled[0:17473,7,0]\n",
    "y2072=yhat_scaled[0:17473,8,0]\n",
    "y2076=yhat_scaled[0:17473,9,0]\n",
    "y2086=yhat_scaled[0:17473,10,0]\n",
    "y2091=yhat_scaled[0:17473,11,0]\n",
    "\n",
    "\n",
    "y1991=scalerY1991.inverse_transform(y1991)\n",
    "y1992=scalerY1992.inverse_transform(y1992)\n",
    "y1998=scalerY1998.inverse_transform(y1998)\n",
    "y1999=scalerY1999.inverse_transform(y1999)\n",
    "y2005=scalerY2005.inverse_transform(y2005)\n",
    "y2006=scalerY2006.inverse_transform(y2006)\n",
    "y2065=scalerY2065.inverse_transform(y2065)\n",
    "y2072=scalerY2072.inverse_transform(y2072)\n",
    "y2086=scalerY2086.inverse_transform(y2086)\n",
    "y2091=scalerY2091.inverse_transform(y2091)\n",
    "y2070=scalerY2070.inverse_transform(y2070)\n",
    "y2076=scalerY2076.inverse_transform(y2076)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSE  & R² Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from math import sqrt\n",
    "rms1 = mean_absolute_error(year_1991_response, y1991)\n",
    "rms2 = mean_absolute_error(year_1992_response, y1992)\n",
    "rms3 = mean_absolute_error(year_1998_response, y1998)\n",
    "rms4 = mean_absolute_error(year_1999_response, y1999)\n",
    "rms5 = mean_absolute_error(year_2005_response, y2005)\n",
    "rms6 = mean_absolute_error(year_2006_response, y2006)\n",
    "rms7 = mean_absolute_error(year_2065_response, y2065)\n",
    "rms8 = mean_absolute_error(year_2070_response, y2070)\n",
    "rms9 = mean_absolute_error(year_2072_response, y2072)\n",
    "rms10= mean_absolute_error(year_2076_response, y2076)\n",
    "rms11 = mean_absolute_error(year_2086_response, y2086)\n",
    "rms12 = mean_absolute_error(year_2091_response, y2091)\n",
    " \n",
    "for i in range(1,13):\n",
    "     print(vars()[\"rms\"+str(i)])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "j=1\n",
    "for i in range(1987,2093):\n",
    "    if(i in [1991,1992,1998,1999,2005,2006,2065,2072,2086,2091,2070,2076]):\n",
    "      vars()[\"coefficient_of_dermination\"+str(j)]= scipy.stats.pearsonr(vars()[\"year_\"+str(i)+\"_response\"].iloc[0:17473,0],vars()[\"y\"+str(i)])[0]**2\n",
    "      j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coefficient_of_dermination1,coefficient_of_dermination2,coefficient_of_dermination3,\n",
    "      coefficient_of_dermination4,coefficient_of_dermination5,coefficient_of_dermination6,coefficient_of_dermination7,\n",
    "     coefficient_of_dermination8,coefficient_of_dermination9,coefficient_of_dermination10,coefficient_of_dermination11,\n",
    "     coefficient_of_dermination12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=1\n",
    "for i in range(1987,2017):\n",
    "    if(i in [1991,1992,1998,1999,2005,2006]):\n",
    "         plt.plot(vars()[\"y\"+str(i)],label=\"Prediction\")\n",
    "         plt.plot(vars()[\"year_\"+str(i)+\"_response\"],label=\"Simulation\")\n",
    "         plt.plot(vars()[\"year_\"+str(i)+\"_response\"][\"MC_OSB\"].to_numpy(),label=\"Simulation\",linewidth=1)\n",
    "         plt.xlabel(\"Days\")\n",
    "         plt.ylabel(\"Daily_Temp_out_OSB\")\n",
    "         plt.title(\"y\"+str(i))\n",
    "         plt.legend(loc=\"upper left\")\n",
    "         plt.text(200,80,'R2 ='+ str(vars()[\"coefficient_of_dermination\"+str(j)])[0:5] )\n",
    "         plt.savefig(\"Year\"+str(i),dpi=1024)\n",
    "         plt.clf()\n",
    "         j=j+1   \n",
    "            \n",
    "\n",
    "j=7         \n",
    "for i in range(2062,2093):\n",
    "    if(i in [2065,2072,2086,2091,2070,2076]):\n",
    "         plt.plot(vars()[\"y\"+str(i)],label=\"Prediction\")\n",
    "         plt.plot(vars()[\"year_\"+str(i)+\"_response\"],label=\"Simulation\")\n",
    "         plt.plot(vars()[\"year_\"+str(i)+\"_response\"][\"MC_OSB\"].to_numpy(),label=\"Simulation\",linewidth=1)\n",
    "         plt.xlabel(\"Days\")\n",
    "         plt.ylabel(\"Daily_RH_out_OSB\")\n",
    "         plt.title(\"y\"+str(i))\n",
    "         plt.legend(loc=\"upper left\")\n",
    "         plt.text(200,80,'R2 ='+ str(vars()[\"coefficient_of_dermination\"+str(j)])[0:5] )\n",
    "         plt.savefig(\"Year\"+str(i),dpi=1024)\n",
    "         plt.clf()\n",
    "         j=j+1\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13",
   "language": "python",
   "name": "tf-gpu4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
